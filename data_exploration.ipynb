{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import sys, os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import folium\n",
    "from folium import plugins\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for output csv file\n",
    "def data_exploration(df_combined, months_folders, output_csv_path, columns_to_select):\n",
    "\n",
    "    # Initialize a counter to track the number of NaN values replaced\n",
    "    nan_replaced_count = 0\n",
    "\n",
    "    for month_folder in months_folders:\n",
    "        # Define the path to the input CSV file\n",
    "        input_csv_path = os.path.join(month_folder, 'listings.csv')\n",
    "        \n",
    "        # Read the CSV file into a Pandas DataFrame\n",
    "        df = pd.read_csv(input_csv_path)\n",
    "        \n",
    "        # Filter the DataFrame to include only the columns that are present in the file\n",
    "        columns_to_keep = [col for col in columns_to_select if col in df.columns]\n",
    "        df_selected = df[columns_to_keep]\n",
    "        \n",
    "        # Initialize a set to store columns from other CSV files\n",
    "        additional_columns = set()\n",
    "        \n",
    "        # Iterate over all CSV files in the folder\n",
    "        for filename in os.listdir(month_folder):\n",
    "            if filename.endswith('.csv') and filename != 'listings.csv':  # Check if it's a CSV file and not the original file\n",
    "                file_path = os.path.join(month_folder, filename)\n",
    "                other_df = pd.read_csv(file_path)\n",
    "\n",
    "                # Iterate over each column in the other DataFrame\n",
    "                for col in other_df.columns:\n",
    "                    # If the column is not already in the selected DataFrame and it's in the columns_to_select list, add it\n",
    "                    if col not in df_selected.columns and col in columns_to_select:\n",
    "                        additional_columns.add(col)\n",
    "                        # Add the column and its data to the selected DataFrame\n",
    "                        df_selected[col] = other_df[col]\n",
    "\n",
    "                    elif  col in df_selected.columns and col in columns_to_select:\n",
    "                        # Check if any value in the selected DataFrame for this column is NaN\n",
    "                        nan_mask = df_selected[col].isna()\n",
    "                        \n",
    "                        # Check if the corresponding value in the other DataFrame is not NaN\n",
    "                        non_nan_mask = ~other_df[col].isna()\n",
    "                        \n",
    "                        # Update the selected DataFrame where NaN values are found\n",
    "                        df_selected.loc[nan_mask & non_nan_mask, col] = other_df.loc[nan_mask & non_nan_mask, col]\n",
    "                        # print(f\"NaN values in column '{col}' replaced with non-NaN values.\")\n",
    "\n",
    "                        # Count the number of NaN values replaced\n",
    "                        nan_replaced_count += sum(nan_mask & non_nan_mask)\n",
    "\n",
    "        # Concatenate the current month's DataFrame with the overall concatenated DataFrame\n",
    "        df_combined = pd.concat([df_combined, df_selected], ignore_index=True)\n",
    "\n",
    "    # Write the concatenated DataFrame to the output CSV file\n",
    "    df_combined.to_csv(output_csv_path, index=False)\n",
    "\n",
    "    print(f\"Data from all months has been written to {output_csv_path}\")\n",
    "    print(f\"The output CSV file has {df_selected.shape[1]} columns.\")\n",
    "    # Print the columns that have been added to the output file\n",
    "    # print(\"Columns added to the output file:\")\n",
    "    # for col in df_selected.columns:\n",
    "    #    print(col)\n",
    "\n",
    "    # Print the number of NaN values replaced\n",
    "    print(f\"Number of NaN values replaced: {nan_replaced_count}\")\n",
    "\n",
    "    return df_combined\n",
    "\n",
    "# Initialize an empty DataFrame to store the concatenated data for 2019\n",
    "df_combined_2019 = pd.DataFrame()\n",
    "\n",
    "# Initialize an empty DataFrame to store the concatenated data for 2023\n",
    "df_combined_2023 = pd.DataFrame()\n",
    "\n",
    "# Define the columns you want to select\n",
    "columns_to_select_2019 = ['id', 'zipcode', 'transit', 'bedrooms', 'beds', 'review_scores_rating', 'number_of_reviews',\n",
    "                        'neighbourhood', 'name', 'latitude', 'longitude', 'last_review', 'instant_bookable', 'host_since',\n",
    "                        'host_response_rate','host_identity_verified', 'host_has_profile_pic', 'first_review', 'description','city',\n",
    "                        'cancellation_policy', 'bed_type', 'bathrooms', 'accommodates', 'amenities', 'room_type',\n",
    "                        'property_type', 'price', 'availability_365', 'minimum_nights', 'last_scraped', 'comments']\n",
    "\n",
    "# Define the columns you want to select\n",
    "columns_to_select_2023 = ['id', 'zipcode', 'transit', 'bedrooms', 'beds', 'review_scores_rating', 'number_of_reviews',\n",
    "                        'neighbourhood_cleansed', 'name', 'latitude', 'longitude', 'last_review', 'instant_bookable', 'host_since',\n",
    "                        'host_response_rate','host_identity_verified', 'host_has_profile_pic', 'first_review', 'description','city',\n",
    "                        'cancellation_policy', 'bed_type', 'bathrooms', 'accommodates', 'amenities', 'room_type',\n",
    "                        'property_type', 'price', 'availability_365', 'minimum_nights', 'last_scraped', 'comments']\n",
    "\n",
    "### 2019 ###\n",
    "\n",
    "# Iterate over each month's folder\n",
    "months_folders = ['data/2019/april', 'data/2019/febrouary', 'data/2019/march']\n",
    "\n",
    "# Define the path to the output CSV file\n",
    "output_csv_path_2019 = 'data_train/train_2019.csv'\n",
    "\n",
    "df_combined_2019 = data_exploration(df_combined_2019, months_folders, output_csv_path_2019, columns_to_select_2019)\n",
    "\n",
    "### 2023 ###\n",
    "\n",
    "# Iterate over each month's folder\n",
    "months_folders = ['data/2023/june', 'data/2023/march', 'data/2023/september']\n",
    "\n",
    "# Define the path to the output CSV file\n",
    "output_csv_path_2023 = 'data_train/train_2023.csv'\n",
    "\n",
    "df_combined_2023 = data_exploration(df_combined_2023, months_folders, output_csv_path_2023, columns_to_select_2023)\n",
    "df_combined_2023.rename(columns={'neighbourhood_cleansed': 'neighbourhood'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning \n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "def cleaning_data(df, output_csv_path):\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΕΜΠΟΡΙΚΟ ΤΡΙΓΩΝΟ-ΠΛΑΚΑ', 'Plaka')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΠΛΑΤΕΙΑ ΑΤΤΙΚΗΣ', 'Attiki')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΜΟΥΣΕΙΟ-ΕΞΑΡΧΕΙΑ-ΝΕΑΠΟΛΗ', 'Exarcheia')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΑΓΙΟΣ ΚΩΝΣΤΑΝΤΙΝΟΣ-ΠΛΑΤΕΙΑ ΒΑΘΗΣ','Agios Konstantinos')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΚΥΨΕΛΗ', 'Kypseli')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΠΑΓΚΡΑΤΙ', 'Pangrati')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΝΕΟΣ ΚΟΣΜΟΣ', 'Neos Kosmos')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΚΟΛΩΝΑΚΙ', 'Kolonaki')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΑΜΠΕΛΟΚΗΠΟΙ', 'Ambelokipi')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΚΟΥΚΑΚΙ-ΜΑΚΡΥΓΙΑΝΝΗ', 'Koukaki')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΚΕΡΑΜΕΙΚΟΣ', 'Kerameikos')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΣΤΑΔΙΟ', 'Stadio')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΑΚΡΟΠΟΛΗ', 'Akropoli')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΛΥΚΑΒΗΤΤΟΣ', 'Lykavittos')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΠΕΝΤΑΓΩΝΟ', 'Pedagono')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΑΚΑΔΗΜΙΑ ΠΛΑΤΩΝΟΣ', 'Akadimia Platonos')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΘΗΣΕΙΟ', 'Thiseio')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΓΟΥΒΑ', 'Gouva')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΓΟΥΔΙ', 'Goudi')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΠΕΔΙΟ ΑΡΕΩΣ', 'Pedion Areos')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΑΝΩ ΠΑΤΗΣΙΑ', 'Patisia')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΚΟΛΩΝΟΣ', 'Kolonos')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΝΙΡΒΑΝΑ', 'Nirvana')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΕΛΛΗΝΟΡΩΣΩΝ', 'Ellinoroson')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΙΛΙΣΙΑ', 'Ilisia')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΖΑΠΠΕΙΟ', 'Zappeio')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΓΚΥΖΗ', 'Gizi')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΒΟΤΑΝΙΚΟΣ', 'Votanikos')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΑΓΙΟΣ ΝΙΚΟΛΑΟΣ', 'Agios Nikolaos')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΠΕΤΡΑΛΩΝΑ', 'Petralona')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΓΚΑΖΙ', 'Gazi')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('1Ο ΝΕΚΡΟΤΑΦΕΙΟ', '1o Nekrotafio')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΠΑΤΗΣΙΑ', 'Patisia')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΠΡΟΜΠΟΝΑ', 'Probona')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΠΟΛΥΓΩΝΟ', 'Poligono')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΑΓΙΟΣ ΕΛΕΥΘΕΡΙΟΣ','Agios Eleftherios')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΠΛΑΤΕΙΑ ΑΜΕΡΙΚΗΣ', 'Plateia Amerikis')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΚΟΛΟΚΥΝΘΟΥ', 'Kolokinthou')\n",
    "    \n",
    "    # specifically for 2023\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΑΝΩ ', '')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΣΤΑΘΜΟΣ ΛΑΡΙΣΗΣ', 'Stathmos Larisis')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΝΕΑ ', '')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΣΕΠΟΛΙΑ', 'Sepolia')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΡΙΖΟΥΠΟΛΗ', 'Rizoupoli')\n",
    "    df['neighbourhood'] = df['neighbourhood'].astype(str).str.replace('ΡΗΓΙΛΛΗΣ', 'Rigilis')\n",
    "\n",
    "\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "cleaning_data(df_combined_2019, output_csv_path_2019)\n",
    "\n",
    "cleaning_data(df_combined_2023, output_csv_path_2023)\n",
    "\n",
    "neighborhood_counts = df_combined_2023['neighbourhood'].value_counts()\n",
    "print(neighborhood_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of file paths with directory prefix\n",
    "file_paths = ['data/2023/june/listings.csv', 'data/2023/june/listings0.csv',\n",
    "               'data/2023/june/reviews.csv', ]\n",
    "\n",
    "# Define the columns to check (excluding 'id')\n",
    "columns_to_check = ['price', 'minimum_nights', 'zipcode', 'transit', 'cancellation_policy', 'bedrooms', 'beds',\n",
    "                    'review_scores_rating', 'number_of_reviews', 'neighbourhood', 'name', 'latitude', 'longitude',\n",
    "                    'last_review', 'instant_bookable', 'host_since', 'host_response_rate', 'host_identity_verified',\n",
    "                    'host_has_profile_pic', 'first_review', 'description', 'city', 'bed_type', 'bathrooms',\n",
    "                    'accommodates', 'amenities', 'room_type', 'property_type', 'availability_365']\n",
    "\n",
    "# Read the data from each file into a dictionary of DataFrames\n",
    "dfs = {}\n",
    "for file_path in file_paths:\n",
    "    df_name = file_path.split('/')[-1].split('.')[0]  # Extracting DataFrame name from file path\n",
    "    dfs[df_name] = pd.read_csv(file_path)\n",
    "\n",
    "# Initialize a list to store inconsistencies\n",
    "inconsistencies = []\n",
    "\n",
    "# Helper function to normalize value for comparison\n",
    "def normalize_value(value):\n",
    "    if isinstance(value, str):\n",
    "        value = value.strip()  # Remove leading/trailing whitespaces for strings\n",
    "        if value.startswith('$'):\n",
    "            value = value[1:]  # Remove leading currency symbol\n",
    "        if '.' in value:\n",
    "            value = value.rstrip('0').rstrip('.')  # Remove trailing zeros and decimal point\n",
    "        return value\n",
    "    elif isinstance(value, float):\n",
    "        return round(value, 2)  # Round float values to 2 decimal places\n",
    "    else:\n",
    "        return value  # Return other types unchanged\n",
    "\n",
    "# Iterate over the 'id' column in the 'listings.csv' DataFrame\n",
    "for idx, row in dfs['listings'].iterrows():\n",
    "    listing_id = row['id']  # Get the id from 'listings.csv'\n",
    "    # Iterate over other DataFrames to check corresponding values for this id\n",
    "    for df_name, df in dfs.items():\n",
    "        if df_name != 'listings':  # Exclude 'listings.csv' itself\n",
    "            # Check if the id exists in the current DataFrame\n",
    "            if 'id' in df.columns and listing_id in df['id'].values:\n",
    "                # Get the row with the matching id\n",
    "                matching_row = df[df['id'] == listing_id].iloc[0]\n",
    "                # Compare values of columns_to_check\n",
    "                for column in columns_to_check:\n",
    "                    if column in row and column in matching_row:\n",
    "                        value1 = normalize_value(row[column])\n",
    "                        value2 = normalize_value(matching_row[column])\n",
    "                        if value1 != value2:\n",
    "                            # Append the inconsistency to the list\n",
    "                            inconsistencies.append((listing_id, column, df_name, value1, value2))\n",
    "\n",
    "# Print any inconsistencies found\n",
    "if inconsistencies:\n",
    "    print(\"Inconsistencies found:\")\n",
    "    for inconsistency in inconsistencies:\n",
    "        print(f\"ID: {inconsistency[0]}, Column: {inconsistency[1]}, File: {inconsistency[2]}\")\n",
    "        print(f\"  Value in listings.csv: {inconsistency[3]}\")\n",
    "        print(f\"  Value in {inconsistency[2]}: {inconsistency[4]}\")\n",
    "else:\n",
    "    print(\"No inconsistencies found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1.1\n",
    "\n",
    "# Function that returns the most common room type of a year\n",
    "def most_common_room_type(df_combined):\n",
    "    result = df_combined['room_type'].value_counts().idxmax()\n",
    "    return result\n",
    "\n",
    "# Year 2019\n",
    "room_type = most_common_room_type(df_combined_2019)\n",
    "print(\"For 2019 the most common room type is:\")\n",
    "print(room_type)\n",
    "\n",
    "# Year 2023\n",
    "room_type = most_common_room_type(df_combined_2023)\n",
    "print(\"\\nFor 2023 the most common room type is:\")\n",
    "print(room_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1.2\n",
    "\n",
    "def average_price(df_combined, output_csv_path):\n",
    "    df_combined['last_scraped'] = pd.to_datetime(df_combined['last_scraped'])\n",
    "\n",
    "    # Extract month from date\n",
    "    df_combined['month'] = df_combined['last_scraped'].dt.month.astype('category')\n",
    "    # Clean the 'price' column by removing commas and dollar signs\n",
    "    df_combined['price'] = df_combined['price'].astype(str).str.replace(',', '').str.replace('$', '')\n",
    "\n",
    "    # Convert the 'price' column to float\n",
    "    df_combined['price'] = df_combined['price'].astype(float)\n",
    "    # Continue with the previous steps to calculate the monthly average price\n",
    "\n",
    "    # Group by month and calculate average price\n",
    "    monthly_avg_price = df_combined.groupby('month')['price'].mean()\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    monthly_avg_price.plot(marker='o', linestyle='-')\n",
    "\n",
    "    # Set labels and title\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Average Price')\n",
    "    plt.title('Average Price Over 3 Months')\n",
    "\n",
    "    # Show grid\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Show plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    df_combined.to_csv(output_csv_path, index=False)\n",
    "\n",
    "# Year 2019\n",
    "print(\"For 2019 the average price over 3 months is\")\n",
    "average_price(df_combined_2019, output_csv_path_2019)\n",
    "\n",
    "# Year 2023\n",
    "print(\"\\nFor 2023 the average price over 3 months is:\")\n",
    "average_price(df_combined_2023, output_csv_path_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1.3\n",
    "\n",
    "# xazo to na katharisoume ta (Athens, Attiki, Greece),(Athens, Attika, Greece) ή (Athina, Attika), (Αθήνα, Attikhs) AFOU \n",
    "# DEN EINAI GEITONIES RE PANAGIOTAKI KAI DEN TA EXOUME AUTA GTXM\n",
    "#pd.set_option('display.max_rows', None)\n",
    "#pd.set_option('display.max_columns', None)\n",
    "#pd.reset_option('display.max_rows')\n",
    "#pd.reset_option('display.max_columns')\n",
    "\n",
    "def top_neighbourhoods(df_combined):\n",
    "    neighbourhood_reviews = df_combined.groupby('neighbourhood')['number_of_reviews'].sum()\n",
    "\n",
    "    neighbourhood_reviews_sorted = neighbourhood_reviews.sort_values(ascending=False)\n",
    "\n",
    "    # Select the top 5 neighbourhoods\n",
    "    top_5_neighbourhoods = neighbourhood_reviews_sorted.head()\n",
    "\n",
    "    # Print the result\n",
    "    print(top_5_neighbourhoods)\n",
    "\n",
    "# Year 2019\n",
    "print(\"Top 5 neighbourhoods for the year 2019:\")\n",
    "top_neighbourhoods(df_combined_2019)\n",
    "\n",
    "# Year 2023\n",
    "print(\"\\nTop 5 neighbourhoods for the year 2023:\")\n",
    "top_neighbourhoods(df_combined_2023)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1.4\n",
    "\n",
    "def find_neighbourhood(df_combined):\n",
    "\n",
    "    temp_df = df_combined.copy()\n",
    "    temp_df['last_scraped'] = pd.to_datetime(temp_df['last_scraped'])\n",
    "\n",
    "    # Extract month from date\n",
    "    temp_df['month'] = temp_df['last_scraped'].dt.month.astype('float')\n",
    "\n",
    "    temp_df.drop_duplicates(subset=['id'], inplace = True)\n",
    "    properties = temp_df.groupby(['neighbourhood','month'])['id'].count()\n",
    "\n",
    "    neighborhood_total_counts = properties.groupby('neighbourhood').sum()\n",
    "\n",
    "    max_property = neighborhood_total_counts.idxmax()\n",
    "    max_number = neighborhood_total_counts.max()\n",
    "    print(max_property,max_number)\n",
    "    # number_of_properties = properties.value_counts().idxmax()\n",
    "\n",
    "# Year 2019\n",
    "print(\"The neighbourhood with the most properties for 2019:\")\n",
    "find_neighbourhood(df_combined_2019)\n",
    "\n",
    "# Year 2023\n",
    "print(\"\\nThe neighbourhood with the most properties for 2023:\")\n",
    "find_neighbourhood(df_combined_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1.5\n",
    "\n",
    "def find_registrations(df_combined):\n",
    "\n",
    "    temp_df = df_combined.copy()\n",
    "    temp_df['last_scraped'] = pd.to_datetime(temp_df['last_scraped'])\n",
    "\n",
    "    # Extract month from date\n",
    "    temp_df['month'] = temp_df['last_scraped'].dt.month.astype('float')\n",
    "\n",
    "    temp_df.drop_duplicates(subset=['id'], inplace = True)\n",
    "    neighborhood_month_counts = temp_df.groupby(['neighbourhood', 'month'])['id'].count()\n",
    "    neighborhood_total_counts = neighborhood_month_counts.groupby('neighbourhood').sum()\n",
    "    #kataxoriseis ana geitonia(diaforetikes mallon?)\n",
    "    print(neighborhood_total_counts)\n",
    "\n",
    "    neighborhood_month_id_counts = df_combined.groupby('month')['id'].count()\n",
    "\n",
    "    print(neighborhood_month_id_counts)\n",
    "\n",
    "# Year 2019\n",
    "print(\"Registrations for 2019:\")\n",
    "find_registrations(df_combined_2019)\n",
    "\n",
    "# Year 2023\n",
    "print(\"\\nRegistrations for 2023:\")\n",
    "find_registrations(df_combined_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1.6\n",
    "# Group the data by 'neighbourhood' and 'month' and count the number of unique neighborhoods\n",
    "# unique_neighborhoods = df_combined.groupby(['neighbourhood', 'month'])['id'].size()\n",
    "# unique_neighborhoods\n",
    "# thelw na pethano den kanw plaka\n",
    "\n",
    "def mk_graph(df_combined):\n",
    "    temp_df = df_combined.copy()\n",
    "    temp_df['last_scraped'] = pd.to_datetime(temp_df['last_scraped'])\n",
    "\n",
    "    # Extract month from date\n",
    "    temp_df['month'] = temp_df['last_scraped'].dt.month.astype('float')\n",
    "\n",
    "    temp_df.drop_duplicates(subset=['id'], inplace = True)\n",
    "    properties = temp_df.groupby(['neighbourhood','month'])['id'].count()\n",
    "\n",
    "    neighborhood_total_counts = properties.groupby('neighbourhood').sum()\n",
    "    # Plot histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(neighborhood_total_counts.index.get_level_values('neighbourhood'), neighborhood_total_counts)\n",
    "\n",
    "    # Rotate the x-axis labels for better readability\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    # Set labels and title\n",
    "    plt.xlabel('Neighbourhood')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Unique Neighbourhoods')\n",
    "\n",
    "    for i, count in enumerate(neighborhood_total_counts):\n",
    "        plt.text(i, count, str(count), ha='center', va='bottom', fontsize=6)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 2019\n",
    "print(\"Graph of 2019\")\n",
    "mk_graph(df_combined_2019)\n",
    "\n",
    "# 2023\n",
    "print(\"\\nGraph of 2023\")\n",
    "mk_graph(df_combined_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1.7\n",
    "## NOMIZO META THA THELEI KAPOS NA EPANAFEREIS TA DIPLA STO DF ISOS KSANADIABAZONTAS TO ARXEIO KAI PERNONTAS TO STO IDIO df_combined\n",
    "\n",
    "def find_roomtypes(df_combined):\n",
    "\n",
    "    df_combined.drop_duplicates(subset=['id'])\n",
    "    room_type_counts = df_combined.groupby('neighbourhood')['room_type'].value_counts()\n",
    "\n",
    "\n",
    "    most_common_room_type = room_type_counts.groupby('neighbourhood').nlargest(1).reset_index(level=1, drop=True)\n",
    "    # result = room_type_counts.groupby('neighbourhood').idxmax()\n",
    "    # result = result.reset_index(level = 0, drop = True)\n",
    "    # print(result)\n",
    "    print(most_common_room_type)\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Year 2019\n",
    "print(\"Year 2019:\")\n",
    "find_roomtypes(df_combined_2019)\n",
    "\n",
    "# Year 2023\n",
    "print(\"\\nYear 2023\")\n",
    "find_roomtypes(df_combined_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1.8\n",
    "\n",
    "def find_most_expensive_roomtype(data):\n",
    "    df_combined = pd.DataFrame(data)\n",
    "\n",
    "    most_expensive = df_combined.groupby('room_type')['price'].value_counts()\n",
    "    # most_expensive_type = most_expensive.groupby('room_type').sum()\n",
    "\n",
    "    result0 = 0\n",
    "    result1 = 0\n",
    "    result2 = 0\n",
    "\n",
    "\n",
    "    for price_count  in most_expensive.items():\n",
    "        if price_count[0][0] == 'Entire home/apt':\n",
    "            result0 = result0 +  price_count[0][1] * price_count[1]\n",
    "\n",
    "        if price_count[0][0] == 'Shared room':\n",
    "            result1 = result1 +  price_count[0][1] * price_count[1]\n",
    "        \n",
    "        if price_count[0][0] == 'Private room':\n",
    "            result2 = result2 +  price_count[0][1] * price_count[1]\n",
    "\n",
    "    if result0 > result1:\n",
    "        if result0 > result2:\n",
    "            print('Entire home/apt',result0,'$')\n",
    "        elif result0 < result2:\n",
    "            print('Private room',result2,'$')\n",
    "    elif result1 > result0:\n",
    "        print('Shared room',result1,'$')\n",
    "    \n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# 2019\n",
    "print(\"The most expensive room type for 2019:\")\n",
    "new_file_path = 'data_train/train_2019.csv'\n",
    "data_from_2019 = pd.read_csv(new_file_path)\n",
    "find_most_expensive_roomtype(data_from_2019)\n",
    "\n",
    "# 2023\n",
    "print(\"The most expensive room type for 2023:\")\n",
    "new_file_path = 'data_train/train_2023.csv'\n",
    "data_from_2023 = pd.read_csv(new_file_path)\n",
    "find_most_expensive_roomtype(data_from_2023)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1.9\n",
    "def folium_map(df):\n",
    "\n",
    "    sample_df = df.sample(n=350)\n",
    "\n",
    "    latitude = sample_df['latitude']\n",
    "    longitude = sample_df['longitude']\n",
    "    room_type = sample_df['room_type']\n",
    "\n",
    "    center_latitude = latitude.mean()\n",
    "    center_longitude = longitude.mean()\n",
    "\n",
    "    # Create the Folium map with a starting zoom level\n",
    "    map = folium.Map(location=[center_latitude, center_longitude], zoom_start=12.5)\n",
    "\n",
    "    for i in range(len(latitude)):\n",
    "        lat = latitude.iloc[i]\n",
    "        lon = longitude.iloc[i]\n",
    "        label = str(room_type.iloc[i])\n",
    "        folium.Marker([lat, lon], popup=label).add_to(map)\n",
    "\n",
    "    # Display the map\n",
    "    display(map)\n",
    "\n",
    "\n",
    "# Year 2019\n",
    "# print(\"Year 2019:\")\n",
    "# folium_map(df_combined_2019)\n",
    "\n",
    "# Year 2023\n",
    "print(\"\\nYear 2023:\")\n",
    "folium_map(df_combined_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1.10\n",
    "\n",
    "def wc_create(df, suffix=''):\n",
    "\n",
    "    stopwords = STOPWORDS\n",
    "\n",
    "    wc = WordCloud(\n",
    "        background_color = 'white',\n",
    "        stopwords=stopwords,\n",
    "        height = 1080,\n",
    "        width = 720\n",
    "    )\n",
    "    \n",
    "    if 'neighbourhood' in df.columns:\n",
    "        neighbourhood_text = ' '.join(df['neighbourhood'].astype(str))\n",
    "        wc.generate(neighbourhood_text)\n",
    "        plt.figure(figsize=(5, 4))\n",
    "        plt.imshow(wc, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        print(\"wordcloud - neighbourhood\")\n",
    "        plt.show()\n",
    "        wc.to_file(f'wordclouds/neighbourhood_{suffix}.png')\n",
    "        \n",
    "    if 'transit' in df.columns:\n",
    "        transit_text = ' '.join(df['transit'].astype(str))\n",
    "        wc.generate(transit_text)\n",
    "        plt.figure(figsize=(5, 4))\n",
    "        plt.imshow(wc, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        print(\"wordcloud - transit\")\n",
    "        plt.show()\n",
    "        wc.to_file(f'wordclouds/transit_{suffix}.png')\n",
    "        \n",
    "    if 'description' in df.columns:\n",
    "        description_text = ' '.join(df['description'].astype(str))\n",
    "        wc.generate(description_text)\n",
    "        plt.figure(figsize=(5, 4))\n",
    "        plt.imshow(wc, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        print(\"wordcloud - description\")\n",
    "        plt.show()\n",
    "        wc.to_file(f'wordclouds/description_{suffix}.png')\n",
    "\n",
    "    if 'comments' in df.columns:\n",
    "        review_text = ' '.join(df['comments'].astype(str))\n",
    "        wc.generate(review_text)\n",
    "        plt.figure(figsize=(5, 4))\n",
    "        plt.imshow(wc, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        print(\"wordcloud - comments\")\n",
    "        plt.show()\n",
    "        wc.to_file(f'wordclouds/review_{suffix}.png')\n",
    "\n",
    "# Year 2019\n",
    "print(\"Year 2019:\")\n",
    "wc_create(df_combined_2019, suffix='2019')\n",
    "\n",
    "# Year 2023\n",
    "print(\"Year 2023:\")\n",
    "wc_create(df_combined_2023, suffix='2023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1.11 \n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "def clean_amenities(df):\n",
    "\n",
    "    \n",
    "    categories = {\n",
    "        \"kitchen\": [\"Kitchen\", \"Microwave\", \"Coffee maker\", \"Refrigerator\", \"Dishwasher\", \"Dishes and silverware\", \"Cooking basics\", \"Oven\", \"Stove\",\\\n",
    "                    \"Full kitchen\", \"BBQ grill\"],\n",
    "        \"accessibility\": [\"Elevator\", \"Buzzer/wireless intercom\",\"Patio or balcony\", \"Wide doorway\", \"Step-free access\", \"Wide clearance to bed\", \"Accessible-height bed\",\\\n",
    "                          \"Wide hallway clearance\", \"Flat path to front door\", \"Wide entryway\", \"Flat path to front door\", \"Well-lit path to entrance\",\\\n",
    "                            \"Accessible-height toilet\", \"Wide clearance to shower\", \"Wheelchair accessible\"],\n",
    "        \"Electricity_and_Technology\": [\"TV\", \"Cable TV\", \"Internet\", \"Wifi\", \"Ethernet connection\", \"Pocket wifi\"],\n",
    "        \"facilities\": [\"Air conditioning\", \"Heating\", \"Hot water\", \"Washer\", \"Dryer\", \"High chair\", \"Bathtub\", \"Indoor fireplace\", \"Private living room\", \"Crib\",\\\n",
    "                       \"Hot tub\", \"toilet\"],\n",
    "        \"kids_friendly\": [\"Family/kid friendly\", \"Children’s books and toys\", \"Game console\"],\n",
    "        \"security\": [\"First aid kit\", \"Safety card\", \"Fire extinguisher\", \"Smoke detector\", \"Carbon monoxide detector\", \"Lock on bedroom door\"],\n",
    "        \"essentials\": [\"Essentials\", \"Shampoo\", \"Extra pillows and blankets\",\"Bed linens\",\"Room-darkening shades\",\"Handheld shower head\"],\n",
    "        \"services\": [\"24-hour check-in\", \"Hangers\", \"Hair dryer\", \"Iron\", \"Laptop friendly workspace\", \"Luggage dropoff allowed\", \"Host greets you\", \"Pack ’n Play/travel crib\",\\\n",
    "                     \"Long term stays allowed\",\"Free parking on premises\", \"Breakfast\", \"Free street parking\", \"Building staff\", \"Doorman\"],\n",
    "        \"extra\": [\"Cleaning before checkout\",\"Other\", \"Single level home\", \"Self check-in\", \"Smoking allowed\", \"Pets allowed\",  \"Paid parking off premises\",  \"Paid parking on premises\"],\n",
    "        \"error\": [\"translation missing: en.hosting_amenity_49\", \"translation missing: en.hosting_amenity_50\", \"Lockbox\"]\n",
    "    }\n",
    "    # amenities_list = []\n",
    "    # for amenity_str in df['amenities']:\n",
    "    # # Remove curly braces at the beginning and end (optional)\n",
    "    #     amenity_str = amenity_str[1:-1]  # Assuming curly braces exist\n",
    "    #     amenities = amenity_str.split(\",\")  # Split by commas\n",
    "    #     for amenity in amenities:\n",
    "    #         amenities_list.append(amenity.strip())  # Remove leading/trailing spaces\n",
    "\n",
    "    # amenities_list = [amenity.strip('\"') for amenity in amenities_list]\n",
    "    # print(amenities_list[:20])  # Print the first 10 extracted amenities\n",
    "\n",
    "    # simplified_amenities = []\n",
    "    # for amenity in amenities_list:\n",
    "    #     simplified_list = []\n",
    "    #     for category, types in categories.items():\n",
    "    #         if amenity in types:\n",
    "    #                 simplified_list.append(category)\n",
    "    #                 break\n",
    "    #     simplified_amenities.append(simplified_list)\n",
    "\n",
    "    #display(simplified_amenities)\n",
    "        # Display the amenities_list and simplified_amenities with index\n",
    "\n",
    "    #display(amenities_list, simplified_amenities)\n",
    "\n",
    "\n",
    "    #display(simplified_amenities)\n",
    "\n",
    "############################ TEST #################################################################\n",
    "    # seen = set()\n",
    "\n",
    "    # # Use list comprehension to iterate over the list of lists and remove duplicates\n",
    "    # result = [x for lst in simplified_amenities for x in lst if not (x in seen or seen.add(x))]\n",
    "\n",
    "    # print(len(result))\n",
    "\n",
    "\n",
    "############################ TEST #################################################################\n",
    "\n",
    "## PROBABLY CORRECT\n",
    "    \n",
    "    simplified_amenities_list = []\n",
    "    for amenity_str in df['amenities']:\n",
    "        amenity_list = amenity_str[1:-1].split(\",\")  # Assuming curly braces exist\n",
    "        simplified_list = []\n",
    "        for amenity in amenity_list:\n",
    "            amenity = amenity.strip().strip('\"')  # Remove leading/trailing spaces and quotes\n",
    "            for category, types in categories.items():\n",
    "                if amenity in types:\n",
    "                    simplified_list.append(category)\n",
    "                    break\n",
    "        simplified_amenities_list.append(simplified_list)\n",
    "\n",
    "    df['simplified_amenities'] = simplified_amenities_list\n",
    "\n",
    "\n",
    "    # prepei meta na ginei amenities = simplified_amenities\n",
    "    # an thes boroume na vgaloume ta duplicates apo ta simplified amenities ( den nomizo na prepei)\n",
    "    # na ftiaxo liga ta diagrammata\n",
    "    # print(df['simplified_amenities'])\n",
    "\n",
    "    # Flatten the list of lists\n",
    "    flattened_amenities = [item for sublist in df['simplified_amenities'] for item in sublist]\n",
    "\n",
    "    # Plot the histogram\n",
    "    plt.hist(flattened_amenities, bins=len(set(flattened_amenities)), color='skyblue', edgecolor='black')\n",
    "    plt.xlabel('Amenities')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Simplified Amenities')\n",
    "    plt.xticks(rotation=90, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # print(df['amenities'])\n",
    "\n",
    "# Year 2019\n",
    "print(\"Year 2019:\")\n",
    "clean_amenities(df_combined_2019)\n",
    "\n",
    "# Year 2023\n",
    "print(\"Year 2023:\")\n",
    "clean_amenities(df_combined_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1.12"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
